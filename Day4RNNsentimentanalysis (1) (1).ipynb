{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Sample Data & Preprocessing"
      ],
      "metadata": {
        "id": "tCCM1tLDFPkv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21J0zfvcD2Wa",
        "outputId": "79aebe30-41b2-4672-d8cd-fce047ca8cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i loved the movie', 'it was a terrible film', 'amazing acting and story', 'worst movie ever', 'best film i have seen', 'not good', 'loved it', 'i hated it']\n"
          ]
        }
      ],
      "source": [
        "# Sample reviews and labels (0 = negative, 1 = positive)\n",
        "sample_reviews = [\n",
        "    \"I loved the movie\",\n",
        "    \"It was a terrible film\",\n",
        "    \"Amazing acting and story\",\n",
        "    \"Worst movie ever\",\n",
        "    \"Best film I have seen\",\n",
        "    \"Not good\",\n",
        "    \"Loved it\",\n",
        "    \"I hated it\"\n",
        "]\n",
        "sample_labels = [1, 0, 1, 0, 1, 0, 1, 0]\n",
        "\n",
        "# Basic text preprocessing\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([ch for ch in text if ch not in string.punctuation])\n",
        "    #print(text)\n",
        "    return text\n",
        "\n",
        "cleaned_reviews = [clean_text(r) for r in sample_reviews]\n",
        "print(cleaned_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Tokenize and Create Vocab\n"
      ],
      "metadata": {
        "id": "OYECh5nZD-Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Tokenize\n",
        "all_words = ' '.join(cleaned_reviews).split()\n",
        "counts = Counter(all_words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: i+1 for i, word in enumerate(vocab)}  # Start at 1\n",
        "print(vocab_to_int)\n",
        "\n",
        "# Convert reviews to sequences of integers\n",
        "reviews_int = [[vocab_to_int[word] for word in review.split()] for review in cleaned_reviews]\n",
        "print(reviews_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NlVC1hyEDAY",
        "outputId": "d5ee6006-f8c5-4843-8ea0-9644f33e8b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 1, 'it': 2, 'loved': 3, 'movie': 4, 'film': 5, 'the': 6, 'was': 7, 'a': 8, 'terrible': 9, 'amazing': 10, 'acting': 11, 'and': 12, 'story': 13, 'worst': 14, 'ever': 15, 'best': 16, 'have': 17, 'seen': 18, 'not': 19, 'good': 20, 'hated': 21}\n",
            "[[1, 3, 6, 4], [2, 7, 8, 9, 5], [10, 11, 12, 13], [14, 4, 15], [16, 5, 1, 17, 18], [19, 20], [3, 2], [1, 21, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Pad Sequences"
      ],
      "metadata": {
        "id": "SQoHQ2iRELz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def pad_features(reviews, seq_length):\n",
        "    features = np.zeros((len(reviews), seq_length), dtype=int)\n",
        "    for i, row in enumerate(reviews):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]  # right-pad\n",
        "    return features\n",
        "\n",
        "seq_length = 10\n",
        "features = pad_features(reviews_int, seq_length)\n",
        "print(features)\n",
        "\n",
        "labels = np.array(sample_labels)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDowupwFEQO7",
        "outputId": "277e5bae-cd87-44e8-c56b-93f8457f257e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  1  3  6  4]\n",
            " [ 0  0  0  0  0  2  7  8  9  5]\n",
            " [ 0  0  0  0  0  0 10 11 12 13]\n",
            " [ 0  0  0  0  0  0  0 14  4 15]\n",
            " [ 0  0  0  0  0 16  5  1 17 18]\n",
            " [ 0  0  0  0  0  0  0  0 19 20]\n",
            " [ 0  0  0  0  0  0  0  0  3  2]\n",
            " [ 0  0  0  0  0  0  0  1 21  2]]\n",
            "[1 0 1 0 1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Create DataLoaders"
      ],
      "metadata": {
        "id": "vUXGczQHET4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "# Convert to tensors\n",
        "feature_tensors = torch.from_numpy(features).long()\n",
        "label_tensors = torch.from_numpy(labels).float()\n",
        "\n",
        "dataset = TensorDataset(feature_tensors, label_tensors)\n",
        "train_loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "5eLwqEBVEaNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Build Simple RNN **Model**"
      ],
      "metadata": {
        "id": "PpPYtZNwEbmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim)  # +1 for padding\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        out, hidden = self.rnn(embedded)\n",
        "        out = self.fc(out[:, -1, :])  # Use output from last timestep\n",
        "        return self.sigmoid(out)\n"
      ],
      "metadata": {
        "id": "x3KILxRlEfEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model"
      ],
      "metadata": {
        "id": "-GE7oRJjEjSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = len(vocab_to_int)\n",
        "embed_dim = 16\n",
        "hidden_dim = 32\n",
        "output_dim = 1\n",
        "\n",
        "model = SentimentRNN(vocab_size, embed_dim, hidden_dim, output_dim)............\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFpm9LDFEpVJ",
        "outputId": "96dd20d8-6bd1-49e5-b2ec-94bace1c2e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.6120\n",
            "Epoch 2/5, Loss: 0.6243\n",
            "Epoch 3/5, Loss: 0.6885\n",
            "Epoch 4/5, Loss: 0.5328\n",
            "Epoch 5/5, Loss: 0.7255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try a Prediction"
      ],
      "metadata": {
        "id": "g0UkQkDBEvv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    sentence = clean_text(sentence)\n",
        "    tokens = [vocab_to_int.get(word, 0) for word in sentence.split()]\n",
        "    padded = pad_features([tokens], seq_length)\n",
        "    input_tensor = torch.from_numpy(padded).long()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor).item()\n",
        "    return \"Positive\" if output >= 0.5 else \"Negative\"\n",
        "\n",
        "print(predict_sentiment(model, \"I absolutely loved it\"))\n",
        "print(predict_sentiment(model, \"This was awful\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDCMAaCjEw6W",
        "outputId": "d713ed92-554e-4d90-d2fc-d39d7229174c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n",
            "Positive\n"
          ]
        }
      ]
    }
  ]
}